---
title: "Classification"
output: html_notebook
---

## Chargement des données et des libraries
```{r}
data_classif <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_clas_app.txt")
data_reg <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_reg_app.txt")

#Suffle data
set.seed(123)
data_classif <- data_classif[sample(nrow(data_classif)), ]
data_reg <- data_reg[sample(nrow(data_reg)), ]

#Affichage des datasets
head(data_classif)
head(data_reg)

```

## Subset selection
### Classification
```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.exhaustive <- regsubsets(y ~ . ,data=data_classif,method='exhaustive', really.big = T)
plot(reg.exhaustive,scale="adjr2", main = "Sélection de variables — méthode exhaustive")
plot(reg.exhaustive,scale="aic", main = "Sélection de variables — méthode exhaustive - AIC")
plot(reg.exhaustive,scale="bic", main = "Sélection de variables — méthode exhaustive - BIC")
par(op)
```

```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.forward <- regsubsets(y ~ . ,data=data_classif,method='forward')
plot(reg.forward,scale="adjr2", main = "Sélection de variables — méthode forward")
plot(reg.forward,scale="aic", main = "Sélection de variables — méthode forward - AIC")
plot(reg.forward,scale="bic", main = "Sélection de variables — méthode forward - BIC")
par(op)
```

```{r}
op <- par(mfrow = c(3,3), mar = c(4,4,2,1))
library('leaps')
reg.back <- regsubsets(y ~ . ,data=data_classif,method='backward')
plot(reg.back,scale="adjr2", main = "Sélection de variables — méthode backward - R2 ajusté")
plot(reg.back,scale="aic", main = "Sélection de variables — méthode backward - AIC")
plot(reg.back,scale="bic", main = "Sélection de variables — méthode backward - BIC")
par(op)
```

```{r}
library('MASS')
fit <- lm(y ~ ., data = data_classif)
sel.AIC <- stepAIC(fit,scope = y ~ ., direction = "both")
summary(sel.AIC)

sel.BIC <- stepAIC(fit,scope = y ~ ., direction = "both", k = log(nrow(data_classif)))
summary(sel.BIC)
```

### Régression
```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.exhaustive <- regsubsets(y ~ . , data = data_reg, method =
    'exhaustive', really.big = T)
plot(reg.exhaustive, scale = "adjr2", main = "Sélection — exhaustive (adjR2)")
plot(reg.exhaustive, scale = "Cp",    main = "Sélection — exhaustive (Cp)")   # remplacer 'aic' par 'Cp'
plot(reg.exhaustive, scale = "bic",   main = "Sélection — exhaustive (BIC)")
par(op)
```

```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.forward <- regsubsets(y ~ . , data = data_reg, method = 'forward')
plot(reg.forward, scale = "adjr2", main = "Sélection — forward (adjR2)")
plot(reg.forward, scale = "Cp",    main = "Sélection — forward (Cp  )")   # remplacer 'aic' par 'Cp'
plot(reg.forward, scale = "bic",   main = "Sélection — forward (BIC  )")
par(op) 
```

```{r} 
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.back <- regsubsets(y ~ . , data = data_reg, method = 'back
ward')
plot(reg.back, scale = "adjr2", main = "Sélection — backward (
adjR2)")
plot(reg.back, scale = "Cp",    main = "Sélection — backward (
Cp  )")   # remplacer 'aic' par 'Cp'
plot(reg.back, scale = "bic",   main = "Sélection — backward (
BIC  )")
par(op)
```

```{r}
library('MASS')
fit <- lm(y ~ ., data = data_reg)
sel.AIC <- stepAIC(fit, scope = y ~ ., direction = "both")
sel.BIC <- stepAIC(fit, scope = y ~ ., direction = "both", k = log(nrow(data_reg)))


```
```{r}
sel.AIC.formula <- formula(sel.AIC)
sel.BIC.formula <- formula(sel.BIC)
sel.AIC.score <- AIC()

```

### Selection d'un modèle de classification par R2 AIC BIC
```{r}
library('MASS')
library('class')
library('randomForest')
library("kernlab")

set.seed(123)
df_shuffled <- data_classif[sample(nrow(data_classif)), ]


# Supprimer les lignes avec valeurs manquantes
df_shuffled <- df_shuffled[complete.cases(df_shuffled), ]

train_part <- nrow(df_shuffled)*(80/100)
df_train <- df_shuffled[1:train_part, ]
df_test <- df_shuffled[(train_part+1):nrow(df_shuffled), ]



x_test <- df_test[, names(df_test) != "y"]
y_test <- df_test$y

x_train <- df_train[, names(df_train) != "y"]
y_train <- df_train$y




# LDA
lda.fit1 <- lda(y ~ ., data = df_train)

# QDA
qda.fit1 <- qda(y ~ ., data = df_train)

# KNN génère directement des prédictions (vecteurs) — on les calcule séparément
knn_preds <- list(
  knn.fit1  = knn(train = x_train, test = x_test, cl = y_train, k = 1),
  knn.fit2  = knn(train = x_train, test = x_test, cl = y_train, k = 3),
  knn.fit5  = knn(train = x_train, test = x_test, cl = y_train, k = 5),
  knn.fit7  = knn(train = x_train, test = x_test, cl = y_train, k = 7),
  knn.fit15 = knn(train = x_train, test = x_test, cl = y_train, k = 15),
  knn.fit20 = knn(train = x_train, test = x_test, cl = y_train, k = 20)
)
# Random Forest
rf.fit1 <- randomForest(as.factor(y) ~ ., data = df_train)

# SVM
svm.rbf <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "rbfdot", C=10, type="C-svc")
svm.poly <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "polydot", C=10, type="C-svc")
svm.vanilla <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "vanilladot", C=10, type="C-svc")

models <- list(
               lda = lda.fit1,
               qda = qda.fit1,
               knn1 = knn.fit1,
               knn3 = knn.fit3,
               knn5 = knn.fit5,
               knn7 = knn.fit7,
               knn15 = knn.fit15,
               knn20 = knn.fit20,
               rf = rf.fit1,
               svm_rbf = svm.rbf,
               svm_poly = svm.poly,
               svm_vanilla = svm.vanilla)

results <- data.frame(Model = character(),
                    Accuracy = numeric(),
                    AIC = numeric(),
                    BIC = numeric(),
                    stringsAsFactors = FALSE)

for (model_name in names(models)) {
  model <- models[[model_name]]
  preds_raw <- tryCatch(predict(model, newdata = df_test), error = function(e) tryCatch(predict(model, x_test), error = function(e2) NA))
  predicted <- as.character(preds_raw)
  Accuracy_value <- if (length(y_test) == 0) NA_real_ else mean(predicted == as.character(y_test), na.rm = TRUE)

  AIC_value <- tryCatch(AIC(model), error = function(e) NA_real_)
  BIC_value <- tryCatch(BIC(model), error = function(e) NA_real_)

  results <- rbind(results, data.frame(Model = model_name,
                                       Accuracy = Accuracy_value,
                                       AIC = AIC_value,
                                       BIC = BIC_value,
                                       stringsAsFactors = FALSE))
}

# Boucle sur KNN (prédictions déjà calculées) — AIC/BIC non applicables => NA
for (knn_name in names(knn_preds)) {
  predicted <- as.character(knn_preds[[knn_name]])
  Accuracy_value <- if (length(y_test) == 0) NA_real_ else mean(predicted == as.character(y_test), na.rm = TRUE)
  results <- rbind(results, data.frame(Model = knn_name,
                                       Accuracy = Accuracy_value,
                                       AIC = NA_real_,
                                       BIC = NA_real_,
                                       stringsAsFactors = FALSE))
}

print(results)

```

## Regularization

### Ridge

```{r}
library(glmnet)
x <- model.matrix(y ~ . , data = df_shuffled)  
y <- df_shuffled$y
train_part <- nrow(df_shuffled)*(80/100)
x_train <- x[1:train_part, ]
y_train <- y[1:train_part]
x_test <- x[(train_part+1):nrow(df_shuffled), ]
y_test <- y[(train_part+1):nrow(df_shuffled)]

cv.out <- cv.glmnet(x_train, y_train, alpha = 0)
plot(cv.out)
fit.ridge <- glmnet(x_train,y_train, lambda=cv.out$lambda.min, alpha=0,  family = "multinomial" )
pred.ridge <- predict(fit.ridge, s= cv.out$lambda.min, newx=x_test)

print(mean((y_test-pred.ridge)^2))

```
### Lasso
```{r}
library(glmnet)
x <- model.matrix(y ~ . , data = df_shuffled)  
y <- df_shuffled$y
train_part <- nrow(df_shuffled)*(80/100)
x_train <- x[1:train_part, ]
y_train <- y[1:train_part]
x_test <- x[(train_part+1):nrow(df_shuffled), ]
y_test <- y[(train_part+1):nrow(df_shuffled)]

cv.out <- cv.glmnet(x_train, y_train, alpha = 1)
plot(cv.out)
fit.lasso <- glmnet(x_train,y_train, lambda=cv.out$lambda.min, alpha=1, family = "multinomial")
pred.lasso <- predict(fit.lasso, s= cv.out$lambda.min, newx=x_test)

accuracy <- mean(pred == y_test)
print(accuracy)

```
```{r}
summary(df_shuffled$y)
```

Ici Ridge et Lasso sont clairement mauvais car classification (à utiliser sur la regression nonobstant)

Résumé on va plutôt comprare
- LDA
- QDA
- KNN
- RF
- SVM
- Regression logistique


## Fonction de validation croisée
```{r}
get_folds <- function(data, folds = 5, seed = 123) {
  set.seed(seed)
  n <- nrow(data)
  data_shuff <- data[sample(n), ]

  fold_size <- floor(n / folds)
  folds_list <- list()

  for (i in 1:folds) {
    start <- (i - 1) * fold_size + 1
    end <- if (i == folds) n else i * fold_size
    folds_list[[i]] <- data_shuff[start:end, ]
}

  return(folds_list)
}

nested_cv <- function(
  data,
  outer_folds = 5,
  inner_folds = 5,
  model_function,
  predict_function,
  score_function,
  hyper_grid
){
  # créer les plis externes
  outer_folds_list <- get_folds(data, outer_folds, 342)

  # résultats externes
  results_outer <- data.frame(
    fold = 1:outer_folds,
    best_param = NA,
    score = NA
  )
  
  #Boucle externe
  for(i in 1:outer_folds){
    # Division du jeu de données externe
    test_outer <- outer_folds_list[[i]]
    train_outer <- do.call(rbind, outer_folds_list[-i])
    
    # créer les plis internes
    inner_folds_list <- get_folds(train_outer, inner_folds, 322 + i)

    # stocker le score moyen de chaque hyperparamètre
    hyper_scores <- data.frame(param = hyper_grid, score = NA)
    
    #Boucler pour tester les paramètres choisis
    for(p in seq_along(hyper_grid)){
      param <- hyper_grid[p]
      inner_scores <- c()  # score pour chaque pli interne
      
      #Boucle interne
      for(j in 1:inner_folds){
        test_inner <- inner_folds_list[[j]]
        train_inner <- do.call(rbind, inner_folds_list[-j])

        # entraîner le modèle avec l'hyperparamètre
        model <- model_function(train_inner, param)

        # prédictions sur le pli interne
        pred <- predict_function(model, test_inner)

        # calcul de la métrique
        inner_scores[j] <- score_function(pred, test_inner$y)
      }

      # score moyen sur tous les plis internes pour ce paramètre
      hyper_scores$score[p] <- mean(inner_scores)
    }

    # choisir le meilleur hyperparamètre
    best_param <- hyper_scores$param[which.max(hyper_scores$score)]

    # entraîner sur tout train_outer avec le meilleur hyperparamètre
    final_model <- model_function(train_outer, best_param)

    # prédiction sur le pli externe
    pred_outer <- predict_function(final_model, test_outer)

    # calculer la performance sur le pli externe
    score_outer <- score_function(pred_outer, test_outer$y)

    # stocker les résultats
    results_outer$best_param[i] <- best_param
    results_outer$score[i] <- score_outer
  }

  return(results_outer)
}
```

## Test de la fonction sur un RF
```{r}
data_classif <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_clas_app.txt")

models_perf <- data.frame(
    model = NA,
    tested_param = NA,
    best_param = NA,
    mean_score = NA
  )

data_classif$y <- as.factor(data_classif$y)

train_rf_mtry <- function(data, mtry_value) {
  library(randomForest)
  model <- randomForest(y ~ ., data = data, mtry = mtry_value)
  return(model)
}

predict_rf <- function(rf, newdata){
  library(randomForest)
  pred <- predict(rf, newdata)
  return(pred)
}

accuracy <- function(pred, y_true){
  return(mean(as.factor(pred) == as.factor(y_true)))
}

rf_grid_mtry <- c(2, 4, 6, 8)

res <- nested_cv(
  data = data_classif,          
  model_function = train_rf_mtry,
  predict_function = predict_rf,
  score_function = accuracy,
  hyper_grid = rf_grid_mtry,
  outer_folds = 5,
  inner_folds = 5
)
models_perf <- rbind(models_perf, data.frame(
    model = "RandomForest",
    tested_param = "mtry",
    best_param = names(sort(table(res$best_param), decreasing = TRUE))[1],
    mean_score = mean(res$score)
  ))


```

## Test sur SVM

```{r}
library(e1071)

data_classif <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_clas_app.txt")

svm_perf <- data.frame(
    model = NA,
    tested_param = NA,
    best_param = NA,
    mean_score = NA
  )

data_classif$y <- as.factor(data_classif$y)


train_svm_cost <- function(data, cost_value) {
  model <- svm(y ~ ., data = data, cost = cost_value)
  return(model)
}

train_svm_kernel <- function(data, kernel_type) {
  model <- svm(y ~ ., data = data, kernel = kernel_type)
  return(model)
}

train_svm_gamma <- function(data, gamma_value) {
  model <- svm(y ~ ., data = data, gamma = gamma_value)
  return(model)
}

train_svm_degree <- function(data, degree_value) {
  model <- svm(y ~ ., data = data, degree = degree_value)
  return(model)
}

train_svm_epsilon <- function(data, epsilon_value) {
  model <- svm(y ~ ., data = data, epsilon = epsilon_value)
  return(model)
}

predict_svm <- function(svm_model, newdata){
  pred <- predict(svm_model, newdata)
  return(pred)
}

Accuracy_value <- function(pred, y_true){
  return(mean(pred == y_true))
}

svm_grid_cost <- c(0.1, 1, 10, 100)
svm_grid_kernel <- c("linear", "polynomial", "radial", "sigmoid")
svm_grid_gamma <- c(0.01, 0.1, 1)
svm_grid_degree <- c(2, 3, 4)
svm_grid_epsilon <- c(0.01, 0.1, 0.5)

res_cost <- nested_cv(
  data = data_classif,          
  model_function = train_svm_cost,
  predict_function = predict_svm,
  score_function = Accuracy_value,
  hyper_grid = svm_grid_cost,
  outer_folds = 5,
  inner_folds = 5
)

res_kernel <- nested_cv(
  data = data_classif,          
  model_function = train_svm_kernel,
  predict_function = predict_svm,
  score_function = Accuracy_value,
  hyper_grid = svm_grid_kernel,
  outer_folds = 5,
  inner_folds = 5
)

res_gamma <- nested_cv(
  data = data_classif,          
  model_function = train_svm_gamma,
  predict_function = predict_svm,
  score_function = Accuracy_value,
  hyper_grid = svm_grid_gamma,
  outer_folds = 5,
  inner_folds = 5
)

res_degree <- nested_cv(
  data = data_classif,          
  model_function = train_svm_degree,
  predict_function = predict_svm,
  score_function = Accuracy_value,
  hyper_grid = svm_grid_degree,
  outer_folds = 5,
  inner_folds = 5
)

res_epsilon <- nested_cv(
  data = data_classif,          
  model_function = train_svm_epsilon,
  predict_function = predict_svm,
  score_function = Accuracy_value,
  hyper_grid = svm_grid_epsilon,
  outer_folds = 5,
  inner_folds = 5
)

svm_perf <- rbind(svm_perf, data.frame(
    model = "SVM",
    tested_param = "cost",
    best_param = names(sort(table(res_cost$best_param), decreasing = TRUE))[1],
    mean_score = mean(res_cost$score)
  ))

svm_perf <- rbind(svm_perf, data.frame(
    model = "SVM",
    tested_param = "kernel",
    best_param = names(sort(table(res_kernel$best_param), decreasing = TRUE))[1],
    mean_score = mean(res_kernel$score)
  ))

svm_perf <- rbind(svm_perf, data.frame(
    model = "SVM",
    tested_param = "gamma",
    best_param = names(sort(table(res_gamma$best_param), decreasing = TRUE))[1],
    mean_score = mean(res_gamma$score)
  ))

svm_perf <- rbind(svm_perf, data.frame(
    model = "SVM",
    tested_param = "degree",
    best_param = names(sort(table(res_degree$best_param), decreasing = TRUE))[1],
    mean_score = mean(res_degree$score)
  ))

svm_perf <- rbind(svm_perf, data.frame(
    model = "SVM",
    tested_param = "epsilon",
    best_param = names(sort(table(res_epsilon$best_param), decreasing = TRUE))[1],
    mean_score = mean(res_epsilon$score)
  ))

print(svm_perf)
```

## Test SVM avec les meilleurs hyperparamètres

```{r}
data_classif <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_clas_app.txt")
df_shuffled <- data_classif[sample(nrow(data_classif)), ]
# Supprimer les lignes avec valeurs manquantes
df_shuffled <- df_shuffled[complete.cases(df_shuffled), ]
train_part <- nrow(df_shuffled)*(80/100)
df_train <- df_shuffled[1:train_part, ]
df_test <- df_shuffled[(train_part+1):nrow(df_shuffled), ]
x_test <- df_test[, names(df_test) != "y"]
y_test <- df_test$y
x_train <- df_train[, names(df_train) != "y"]
y_train <- df_train$y

best_svm.model <- svm(y ~ ., data = df_train, cost = 1, kernel = "radial", gamma = 0.01, degree = 2, epsilon = 0.01)
best_svm.pred <- predict(best_svm.model, x_test)
best_svm.accuracy <- mean(best_svm.pred == y_test)
print(best_svm.accuracy)
```

