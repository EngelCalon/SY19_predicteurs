---
title: "Classification"
output: html_notebook
---

## Chargement des données et des libraries
```{r}
data_classif <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_clas_app.txt")
data_reg <- read.table("D:/OneDrive/Documents/Travail/SY19/SY19_predicteurs/src/data/TP5_a25_reg_app.txt")

#Suffle data
set.seed(123)
data_classif <- data_classif[sample(nrow(data_classif)), ]
data_reg <- data_reg[sample(nrow(data_reg)), ]

#Affichage des datasets
head(data_classif)
head(data_reg)

```

## Subset selection
### Classification
```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.exhaustive <- regsubsets(y ~ . ,data=data_classif,method='exhaustive', really.big = T)
plot(reg.exhaustive,scale="adjr2", main = "Sélection de variables — méthode exhaustive")
plot(reg.exhaustive,scale="aic", main = "Sélection de variables — méthode exhaustive - AIC")
plot(reg.exhaustive,scale="bic", main = "Sélection de variables — méthode exhaustive - BIC")
par(op)
```

```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.forward <- regsubsets(y ~ . ,data=data_classif,method='forward')
plot(reg.forward,scale="adjr2", main = "Sélection de variables — méthode forward")
plot(reg.forward,scale="aic", main = "Sélection de variables — méthode forward - AIC")
plot(reg.forward,scale="bic", main = "Sélection de variables — méthode forward - BIC")
par(op)
```

```{r}
op <- par(mfrow = c(3,3), mar = c(4,4,2,1))
library('leaps')
reg.back <- regsubsets(y ~ . ,data=data_classif,method='backward')
plot(reg.back,scale="adjr2", main = "Sélection de variables — méthode backward - R2 ajusté")
plot(reg.back,scale="aic", main = "Sélection de variables — méthode backward - AIC")
plot(reg.back,scale="bic", main = "Sélection de variables — méthode backward - BIC")
par(op)
```

```{r}
library('MASS')
fit <- lm(y ~ ., data = data_classif)
sel.AIC <- stepAIC(fit,scope = y ~ ., direction = "both")
summary(sel.AIC)

sel.BIC <- stepAIC(fit,scope = y ~ ., direction = "both", k = log(nrow(data_classif)))
summary(sel.BIC)
```

### Régression
```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.exhaustive <- regsubsets(y ~ . , data = data_reg, method =
    'exhaustive', really.big = T)
plot(reg.exhaustive, scale = "adjr2", main = "Sélection — exhaustive (adjR2)")
plot(reg.exhaustive, scale = "Cp",    main = "Sélection — exhaustive (Cp)")   # remplacer 'aic' par 'Cp'
plot(reg.exhaustive, scale = "bic",   main = "Sélection — exhaustive (BIC)")
par(op)
```

```{r}
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.forward <- regsubsets(y ~ . , data = data_reg, method = 'forward')
plot(reg.forward, scale = "adjr2", main = "Sélection — forward (adjR2)")
plot(reg.forward, scale = "Cp",    main = "Sélection — forward (Cp  )")   # remplacer 'aic' par 'Cp'
plot(reg.forward, scale = "bic",   main = "Sélection — forward (BIC  )")
par(op) 
```

```{r} 
op <- par(mfrow = c(1,3), mar = c(4,4,2,1))
library('leaps')
reg.back <- regsubsets(y ~ . , data = data_reg, method = 'back
ward')
plot(reg.back, scale = "adjr2", main = "Sélection — backward (
adjR2)")
plot(reg.back, scale = "Cp",    main = "Sélection — backward (
Cp  )")   # remplacer 'aic' par 'Cp'
plot(reg.back, scale = "bic",   main = "Sélection — backward (
BIC  )")
par(op)
```

```{r}
library('MASS')
fit <- lm(y ~ ., data = data_reg)
sel.AIC <- stepAIC(fit, scope = y ~ ., direction = "both")
sel.BIC <- stepAIC(fit, scope = y ~ ., direction = "both", k = log(nrow(data_reg)))


```
```{r}
sel.AIC.formula <- formula(sel.AIC)
sel.BIC.formula <- formula(sel.BIC)
sel.AIC.score <- AIC()

```

### Selection d'un modèle de classification par R2 AIC BIC
```{r}
library('MASS')
library('class')
library('randomForest')
library("kernlab")

df_shuffled <- data_classif[sample(nrow(data_classif)), ]
set.seed(123)

df_train <- df_shuffled[1:800, ]
df_test <- df_shuffled[801:nrow(df_shuffled), ]

x_test <- df_test[, -1]
y_test <- df_test$y

x_train <- df_train[, -1]
y_train <- df_train$y

# Regression logistiques
reglog.fit1 <- glm(y ~ ., data = df_train, family = binomial)

# LDA
lda.fit1 <- lda(y ~ ., data = df_train)

# QDA
qda.fit1 <- qda(y ~ ., data = df_train)

# KNN
knn.fit1 <- knn(train = x_train, test = x_test, cl = y_train, k = 1)
knn.fit3 <- knn(train = x_train, test = x_test, cl = y_train, k = 3)
knn.fit5 <- knn(train = x_train, test = x_test, cl = y_train, k = 5)
knn.fit7 <- knn(train = x_train, test = x_test, cl = y_train, k = 7)
knn.fit15 <- knn(train = x_train, test = x_test, cl = y_train, k = 15)
knn.fit20 <- knn(train = x_train, test = x_test, cl = y_train, k = 20)

# Random Forest
rf.fit1 <- randomForest(as.factor(y) ~ ., data = df_train)

# SVM
svm.rbf <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "rbfdot", C=10, type="C-svc")
svm.poly <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "polydot", C=10, type="C-svc")
svm.vanilla <- ksvm(as.factor(y) ~ ., data = df_train, kernel = "vanilladot", C=10, type="C-svc")

models <- list(reglog = reglog.fit1,
               lda = lda.fit1,
               qda = qda.fit1,
               knn1 = knn.fit1,
               knn3 = knn.fit3,
               knn5 = knn.fit5,
               knn7 = knn.fit7,
               knn15 = knn.fit15,
               knn20 = knn.fit20,
               rf = rf.fit1,
               svm_rbf = svm.rbf,
               svm_poly = svm.poly,
               svm_vanilla = svm.vanilla)

results <- data.frame(Model = character(),
                    Accuracy = numeric(),
                    AIC = numeric(),
                    BIC = numeric(),
                    stringsAsFactors = FALSE)

for (model_name in names(models)) {
  model <- models[[model_name]]
  AIC_value <- AIC(model)
  BIC_value <- BIC(model)
  Accuracy_value <- NA
    if (model_name %in% c("reglog", "lda", "qda")) {
        predictions <- predict(model, newdata = df_test)
        if (model_name == "reglog") {
        predicted_classes <- ifelse(predictions$fitted.values > 0.5, 1, 0)
        } else {
        predicted_classes <- predictions$class
        }
        Accuracy_value <- mean(predicted_classes == y_test)
    } else if (grepl("knn", model_name)) {
        predicted_classes <- model
        Accuracy_value <- mean(predicted_classes == y_test)
    } else if (model_name == "rf") {
        predictions <- predict(model, newdata = df_test)
        Accuracy_value <- mean(predictions == y_test)
    } else if (grepl("svm", model_name)) {
        predictions <- predict(model, newdata = df_test)
        Accuracy_value <- mean(predictions == y_test)
    }
    results <- rbind(results, data.frame(Model = model_name,
                                         Accuracy = Accuracy_value,
                                         AIC = AIC_value,
                                         BIC = BIC_value))
}
print(results)

```